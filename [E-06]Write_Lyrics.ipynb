{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2bc9c3-a6f3-430f-aa84-cc16b5f3d36e",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58fab14-2b67-4ec1-a9c6-07e4ad0d5bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4843aa-5e36-491f-a0f4-cc8b110b7437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7518ea-21b7-4864-b0d3-e6146c916318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df8cfc2f-01f9-43b0-8e44-8e36dbf3873b",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a56023-f2d6-4dc3-a9b7-d6c788604eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*' #os.getenv(x)함수는 환경 변수x의 값을 포함하는 문자열 변수를 반환합니다. txt_file_path 에 \"/root/aiffel/lyricist/data/lyrics/*\" 저장\n",
    "\n",
    "txt_list = glob.glob(txt_file_path) #txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당\n",
    "\n",
    "raw_corpus = [] \n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
    "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ee8f3-6828-4480-9d76-d1f23ffa1b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b5feef-20a6-46f4-b5b7-7f04869c94af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a97d2ed8-f478-4cfd-81b4-7455559b10ee",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188bc9ce-f8e2-4845-aaa7-71e198873dfa",
   "metadata": {},
   "source": [
    "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
    "\n",
    "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
    "\n",
    "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 너무 긴 문장은 노래 가사 작사하기에 어울리지 않을 수도 있겠죠.\n",
    "그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기 를 권합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081a5af-1913-4319-9fcf-afc237987a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'([?.!,])', r' \\1 ', sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r'[^a-zA-Z?.!,]+', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d73e9-3f9e-4e50-af3a-f1c8a6e5ef8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f9602-94b0-41a3-b2b5-59a024db2d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "528885b5-9fa7-4fe6-aaf8-1848a6332739",
   "metadata": {},
   "source": [
    "## Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d20510-fc29-41f7-b3c9-3b1636998f2b",
   "metadata": {},
   "source": [
    "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상 으로 설정하세요! 총 데이터의 20% 를 평가 데이터셋으로 사용해 주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534c7c4c-d54d-466a-b617-0a2ac2f0fb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d9b2a-5c8b-4608-af14-eb47ceae4ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "038e1d1e-c658-42f7-b14a-eee66deed7a0",
   "metadata": {},
   "source": [
    "## Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72030a57-9102-4c87-aa92-68057ac39a6c",
   "metadata": {},
   "source": [
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요!\n",
    "\n",
    "잘 설계한 모델을 학습하려면, model.fit() 함수를 사용해야 합니다. model.fit() 함수에는 다양한 인자를 넣어주어야 하는데, 가장 기본적인 인자로는 데이터셋과 epochs가 있습니다. '5. 실습 (2) 인공지능 학습시키기'에서의 예시와 같이 말이죠.\n",
    "\n",
    "model.fit(dataset, epochs=30)\n",
    "하지만 model.fit() 함수의 epochs를 아무리 크게 넣는다 해도 val_loss 값은 2.2 아래로 떨어지지 않습니다. 이럴 경우는 batch size를 변경하는 것과 같이 model.fit() 함수에 다양한 인자를 넣어주면 해결될 수도 있습니다. 자세한 내용은 https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit 를 참고하세요!\n",
    "\n",
    "Loss는 아래 제시된 Loss 함수를 그대로 사용하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a05f3b3a-c2e4-4fd9-969c-674109f816b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Loss\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# tf.keras.losses.SparseCategoricalCrossentropy : https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy( \n\u001b[1;32m      4\u001b[0m     from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# 클래스 분류 문제에서 softmax 함수를 거치면 from_logits = False(default값),그렇지 않으면 from_logits = True.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#Loss\n",
    "# tf.keras.losses.SparseCategoricalCrossentropy : https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( \n",
    "    from_logits=True, reduction='none') # 클래스 분류 문제에서 softmax 함수를 거치면 from_logits = False(default값),그렇지 않으면 from_logits = True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8158bcac-5e05-454f-800c-d330d2231393",
   "metadata": {},
   "source": [
    "마지막으로 멋진 모델이 생성한 가사 한 줄을 제출하시길 바랍니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56729a-c6dc-4a41-826f-0fd5edfb2da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)\n",
    "# generate_text 함수에 lyricist 라 정의한 모델을 이용해서 ilove 로 시작되는 문장을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c88062a-cb91-4a42-b0f5-faa81ad47516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
